version: "3.3"
services:
  spark-master:
    image: spark-master:2.3.1
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8079:8080"
      - "7077:7077"
    networks: 
      default:
        ipv4_address: 172.18.1.2
    volumes:
       - /mnt/spark-apps:/opt/spark-apps
       - /mnt/spark-data:/opt/spark-data
    environment:
      - "SPARK_LOCAL_IP=spark-master"
  spark-worker-1:
    image: spark-worker:2.3.1
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    env_file: ./env/spark-worker.sh
    environment:
      - "SPARK_LOCAL_IP=spark-worker-1"
    networks: 
      default:
        ipv4_address: 172.18.1.3
    volumes:
       - /mnt/spark-apps:/opt/spark-apps
       - /mnt/spark-data:/opt/spark-data
  spark-worker-2:
    image: spark-worker:2.3.1
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    env_file: ./env/spark-worker.sh
    environment:
      - "SPARK_LOCAL_IP=spark-worker-2"
    networks: 
      default:
        ipv4_address: 172.18.1.4
    volumes:
        - /mnt/spark-apps:/opt/spark-apps
        - /mnt/spark-data:/opt/spark-data
networks:
  default:
     external:
      name: hadoopnet 

